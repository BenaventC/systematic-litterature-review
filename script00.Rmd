---
title: "script00"
author: "cb"
date: "`r Sys.Date()`"
output: html_document
---


```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE, warning=FALSE)
library(tidyverse)
library(Rtsne)
library(ggrepel)
library(ggwordcloud)

library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)

library(word2vec)
library(Rtsne)

library(ape)
library(cowplot)

theme_set(theme_minimal())
```

## Data

Une recherche sur (Marketing | consumer) & ( NLP | text analusis) sur Business Source Premier et quelques ajustements.

Un corpus de 110 papiers pour démarrer. n= `nrow(df` .


```{r data}
df <- read_csv("NLP Marketing and consumer - dataforR.csv") %>% 
  filter(!is.na(text))

```

## tables exploration


```{r pressure, echo=FALSE}


t0 <-as.data.frame(prop.table(table(df$review)))
g01<-ggplot(t0,aes(x=reorder(Var1, Freq), y=Freq))+geom_bar(stat="identity")+coord_flip()+labs( title="Nombre d'articles par revue", y="Proportion", x= NULL)

t1<-as.data.frame(table(df$year))

g02<-ggplot(t1, aes(x=Var1, y=Freq, group=1))+ geom_smooth(color="Grey70", size=2)+
  geom_line(stat="identity", size=1.1) + 
  labs( title="Nombre de publications par an", y="Nombre", x=NULL)
g01
plot_grid(g01, g02, labels = c('A', 'B'), label_size = 10, ncol=2,  rel_widths =  c(2,1))

ggsave(filename="quant.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")


```

## Keywords

Cette analyse demande un fine tuning manuel sur les redondances. Il doit être qualitatif, on ne doit pas perdre une nuance mais réduire au plus les singletons.


```{r user}

user2<- df %>% 
  select(id, keywords)%>% 
  mutate(keywords=tolower(keywords))%>%
  separate(keywords, sep=",", into=c("A1","A2","A3","A4","A5","A6","A7","A8","A9","A10", "A11", "A12", "A13", "A14", "A15", "A16"))%>%
  pivot_longer(-id, names_to="Rang", values_to = "keywords") %>%
  filter(!is.na(keywords)) 


user2$keywords= str_trim(user2$keywords,side ="both")

user2$keywords[user2$keywords=="artificial intelligence (ai)"]<-"artificial intelligence"
user2$keywords[user2$keywords=="automated analysis of text"]<-"automated text analysis"
user2$keywords[user2$keywords=="automated textual analysis"]<-"automated text analysis"
user2$keywords[user2$keywords=="natural language processinf"]<-"natural language processing"
user2$keywords[user2$keywords=="natural language processing (nlp)"]<-"natural language processing"
user2$keywords[user2$keywords=="natural language processing (nlp)-based approach"]<-"natural language processing"

user2$keywords[user2$keywords=="nlp tools"]<-"nlp"
user2$keywords[user2$keywords=="online review"]<-"online reviews"
user2$keywords[user2$keywords=="online shopping review"]<-"online reviews"
user2$keywords[user2$keywords=="review"]<-"online reviews"
user2$keywords[user2$keywords=="reviews"]<-"online reviews"

# a compléter plus systématiquement




foo<- user2 %>%mutate(n=1) %>%
  group_by(id,keywords)%>%
  summarise(n=sum(n))

foo1<- foo %>%mutate(m=1) %>%
  group_by(id)%>%
  summarise(m=sum(m))

ggplot(foo1,aes(x=m))+geom_histogram(binwidth = 1)+
  labs(title="Nombre de mots clés par article")


foo2<-user2 %>%
  select(-Rang)%>% 
  group_by(id,keywords)%>%
  summarize(n=1)

foo2<- foo2 %>% filter(nchar(keywords)>1)

foo2<-foo2%>%group_by(id, keywords)%>%summarise(n=sum(n))

foo2<- foo2 %>% pivot_wider(id,names_from = "keywords", values_from ="n" )

foo2<- foo2 %>% replace(is.na(.),0)




foo3<- foo %>% 
  group_by(keywords)%>%
  summarize(n=n())%>%
  rename(word=keywords)

#projection tsne

foo2<-t(foo2)
tsne_out <- Rtsne(foo2,perplexity = 5, dim=2,  check_duplicates = FALSE) # Run TSNE
tsne_out2<-as.data.frame(tsne_out$Y)
word<-rownames(foo2)

tsne_out3<-cbind(tsne_out2, word) 
tsne_out3<- merge(tsne_out3,foo3)

#foo2

tsne_out3%>% filter(n>0) %>%
  ggplot(aes(x=V1, y=V2, label=word))+
  geom_point(aes(size=n), alpha=.5)+
  geom_text_repel(aes(label=ifelse(n>2,word,""),size=2,  max.overlaps =50))
  


ggsave(filename="keywords.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")




```


## Niveau article


```{r collocation}

df$Texte<-paste(df$title, " . ", df$text, df$keywords)

corp<-corpus(df$Texte)

toks<- tokens(corp, remove_punct = TRUE,tolower=TRUE)

tstat_col_caps <- toks %>% 
                  textstat_collocations(min_count = 3)%>%arrange(desc(count))

head(tstat_col_caps, 50)

```

question : comment intégrer les collocations dans le traitement des POS ? 

Une solution simple, prendre en compte les adjectif

```{r pos}

library(udpipe)
#annotations
eng <- udpipe_download_model(language = "english") #ne faire que la première fois
udmodel<- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")

t1<-Sys.time()
UD <- udpipe_annotate(udmodel, x=df$Texte, trace =20)
t2<-Sys.time()
t<-t2-t1
print(t)

UD <- as.data.frame(UD)

foo<-UD %>%
  select(doc_id,paragraph_id, sentence, token_id,token,lemma, upos,feats, dep_rel,head_token_id)

#flextable(head(foo))

```


```{r vecteur}

#on filtre adverbes adjectifs verb et non communs
updated_vocab <- foo %>%  
  filter(upos %in% c('NOUN', 'PROPN', "ADJ")) %>% mutate(lemma=tolower(lemma))

updated_vocab2<- updated_vocab %>%
  group_by(lemma)%>%
  summarise(n=n())

#on reconstitue le texte filtré
text2<-updated_vocab %>%
 group_by(doc_id) %>%
 summarise(description = paste(lemma, collapse = " "))


#on vectorise
set.seed(123456789)
model <- word2vec(x = text2$description, 
                  type = "skip-gram", 
                  window = 5, 
                  dim = 200, 
                  iter = 100,
                  )
embedding <- as.matrix(model)

#test sur review
lookslike <- predict(model, c("nlp"), type = "nearest", top_n = 20)
lookslike


```

## clustering

```{r hclust1}

#on typologise des termes

library(fastcluster) #pour aller plus vite
distance<-as.dist(1 - cor(t(embedding)))
arbre <- hclust(distance, method = "ward.D2")
plot(arbre,  xlab = "", ylab = "", sub = "", axes = FALSE, hang = -1)
rect.hclust(arbre,12, border = "green3")

group<- as.data.frame(cutree(arbre, k = 12))
group<- group %>% 
  rownames_to_column(var="lemma")%>%
  rename(group=2)%>%
  left_join(updated_vocab2, by="lemma")



library(ggwordcloud)
ggplot(group, aes(label = lemma, size = n, color=n)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 10) +facet_wrap(vars(group), ncol=3)

ggsave(filename="cluster_word.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")

arbreg<- as.dendrogram(arbre)
plot(arbreg,  xlab = "Height", horiz = TRUE)

plot(as.phylo(arbre), type = "unrooted", cex = 0.6,
     no.margin = TRUE)
plot(as.phylo(arbre), type = "fan", cex=.5)



tsne_out <- Rtsne(embedding,perplexity = 20, dim=2) # Run TSNE
tsne_out2<-as.data.frame(tsne_out$Y)
#lemma<-rownames(embedding)

tsne_out3<-cbind(tsne_out2,group) %>%
  left_join(updated_vocab2) %>% filter(n>2)

library(ggrepel)
tsne_out3%>%
  ggplot(aes(x=V1, y=V2, label=lemma))+
  geom_text_repel(aes(label=lemma, size=n,color=as.factor(group)),max.overlaps=30)+
    labs(title="Projection 2D du vocabulaire vectorisé des 106 articles 'NLP & Marketing' ",
       subtitle="Pipe : annot. syntax -> word2vec: 200 vecteurs-> hclus ->rtsne",
       x= NULL, y=NULL)+ scale_color_discrete()+  theme(legend.position = "none") 

  


ggsave(filename="vector_word.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")

```


```{r vector4, echo=FALSE}

#titre<-UD %>%group_by

x      <- data.frame(doc_id           = text2$doc_id, 
                     text             = text2$description, 
                     stringsAsFactors = FALSE)

x$text <- txt_clean_word2vec(x$text, tolower=TRUE)


emb <- as.data.frame(doc2vec(model, x$text,  split = " ",type = "embedding"))%>%drop_na()

df<-df %>%
  mutate(Author=paste0(str_extract(auteurs, "[^;]+")," ",year),
         Author=paste0(str_extract(Author, "[^,]+")," ",year))

#on typologise des termes
library(fastcluster) #pour aller plus vite

distance<-as.dist(1 - cor(t(emb)))


arbre <- hclust(dist(distance), method = "ward.D")

plot(arbre,  xlab = "", ylab = "", sub = "", axes = FALSE, hang = -1)
rect.hclust(arbre, 5, border = "green3")
group<- cutree(arbre, k =5)

tsne_out <- Rtsne(emb,perplexity = 5, dim=2, check_duplicates=FALSE) # Run TSNE
tsne_out2<-as.data.frame(tsne_out$Y)

foo<-UD%>%
  group_by(doc_id)%>%
  summarise(title=first(sentence))%>%
  mutate(sentence2=str_sub(title,1,80))

tsne_out3<-cbind(tsne_out2,group, foo,df[,35]) 



means <- tsne_out3 %>%
    group_by(group) %>%
    summarise(mean_F1 = mean(V1),
              mean_F2 = mean(V2))

tsne_out3%>%
  ggplot(aes(x=V1, y=V2, group=group))+
  geom_point(aes(color=as.factor(group)), alpha=.5)+
  geom_text_repel(aes(label=str_wrap(Author,30)),  size=1.5) +
  labs(title="Projection 2D des 106 articles 'NLP & Marketing' vectorisés",
       subtitle="Pipe : annot. syntax -> word2vec: 200 vecteurs-> doc2vec-> hclus ->rtsne",
       x= NULL, y=NULL)+ scale_color_discrete()+  theme(legend.position = "none")


ggsave(filename="vector_article.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")


```

```{r vector5}

vector_tot<-rbind(embedding, emb)
```
